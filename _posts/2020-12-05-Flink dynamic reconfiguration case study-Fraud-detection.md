---

layout:     post
title:      "Dynamic Reconfiguration for Fraud Detection"
subtitle:   ""
date:       2020-12-05
author:     "MYC"
header-img: "img/post-bg-universe.jpg"
catalog: true
tags:
    - streaming
---


## Dynamic Reconfiguration for Fraud Detection In Flink

To fulfill complex business requirements, dynamic reconfigurations are proposed for different purposes. Typical reconfigurations cases are:

1. **Dynamic updates of application logic**
2. **Dynamic data partitioning**
3. **Custom window management**

The high-level goal of the Fraud Detection engine is to consume a stream of financial transactions and evaluate them against a set of rules. These rules are subject to frequent changes and tweaks. In a real production system, it is important to be able to add and remove them at runtime, without incurring an expensive penalty of stopping and restarting the job.

Flink has proposed an example of Fraud Detection system architecture. The Backend exposes a REST API to the Frontend for creating/deleting rules as well as issuing control commands for managing the demo execution. It then relays those Frontend actions to Flink by sending them via a “Control” Kafka topic. The Backend additionally includes a *Transactions Generator* component, which sends an emulated stream of money transfer events to Flink via a separate “Transactions” topic. Alerts generated by Flink are consumed by the Backend from “Alerts” topic and relayed to the UI via WebSockets.

![Figure 2: Demo Components](https://flink.apache.org/img/blog/2019-11-19-demo-fraud-detection/architecture.png)

### Dynamic data partitioning

Keying a stream shuffles all the records such that elements with the same key are assigned to the same partition. This means all records with the same key are processed by the same physical instance of the next operator.

In a typical streaming application, the choice of key is fixed, determined by some static field within the elements. However, in the case of an application striving to provide flexibility in business logic at runtime, this is not enough. In Fraud Detection, different rules may have different specified keys such as:

"*Whenever the **sum** of the accumulated **payment amount** from the same **payer** to the same **beneficiary** within the **duration of a week** is **greater** than **1 000 000 $** - fire an alert.*"

Accordingly, we will use the following simple JSON format to define the aforementioned parameters:

```json
{
  "ruleId": 1,
  "ruleState": "ACTIVE",
  "groupingKeyNames": ["payerId", "beneficiaryId"],
  "aggregateFieldName": "paymentAmount",
  "aggregatorFunctionType": "SUM",
  "limitOperatorType": "GREATER",
  "limit": 1000000,
  "windowMinutes": 10080
}
```

Thus to address this problem, before key mapping to different tasks, we may have a dynamic key distribution function to define different key fields, and keyby will use the corresponding key fields accordingly.

```java
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy(/* some key selector */);
        .process(/* actual calculations and alerting */)
```

The main logic can be shown as follows:

![Figure 3: Forking events with Dynamic Key Function](https://flink.apache.org/img/blog/2019-11-19-demo-fraud-detection/shuffle_function_1.png)

### Dynamic updates of application logic

Dynamic update the application logic in Fraud Detection is to apply various rules on transactions.

`DynamicKeyFunction` provides dynamic data partitioning while `DynamicAlertFunction` is responsible for executing the main logic of processing transactions and sending alert messages according to defined rules.

```java
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy((keyed) -> keyed.getKey());
        .process(new DynamicAlertFunction())
```

There is a list of rules to be used for the dynamic altert. The normal way of updating the logic in application is to modify the variable in app, and recompile it. However, in a real Fraud Detection system, rules are expected to change on a frequent basis, making this approach unacceptable from the point of view of business and operational requirements. A different approach is needed.

```mjava
public class DynamicKeyFunction
    extends ProcessFunction<Transaction, Keyed<Transaction, String, Integer>> {

  /* Simplified */
  List<Rule> rules = /* Rules that are initialized somehow.*/;
  ...
}
```

To achieve this, one approach specified in Flink is to use [broadcast data distribution mechanism](https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/stream/state/broadcast_state.html).

![Figure 2: Job Graph of the Fraud Detection Flink Job](https://flink.apache.org/img/blog/patterns-blog-2/job-graph.png)

The main blocks of the Transactions processing pipeline are:

- **Transaction Source** that consumes transaction messages from Kafka partitions in parallel.
- **Dynamic Key Function** that performs data enrichment with a dynamic key. The subsequent `keyBy` hashes this dynamic key and partitions the data accordingly among all parallel instances of the following operator.
- **Dynamic Alert Function** that accumulates a data window and creates Alerts based on it.

In my understanding, the rules source here is a control stream, where users can specify the rules, and the dynamic Alert function and dynamic key function can consume the control stream to update their logic.

We argue that this part is still somehow heavy for end users, the system can provide a control plane to provide the logic update APIs for users.

```java
// Streams setup
DataStream<Transaction> transactions = [...]
DataStream<Rule> rulesUpdateStream = [...]

BroadcastStream<Rule> rulesStream = rulesUpdateStream.broadcast(RULES_STATE_DESCRIPTOR);

// Processing pipeline setup
 DataStream<Alert> alerts =
     transactions
         .connect(rulesStream)
         .process(new DynamicKeyFunction())
         .keyBy((keyed) -> keyed.getKey())
         .connect(rulesStream)
         .process(new DynamicAlertFunction())
```

The broadcast stream can be created from any regular stream by calling the `broadcast` method and specifying a state descriptor. Flink assumes that broadcasted data needs to be stored and retrieved while processing events of the main data flow and, therefore, always automatically creates a corresponding *broadcast state* from this state descriptor. This is different from any other Apache Flink state type in which you need to initialize it in the `open()` method of the processing function. Also note that broadcast state always has a key-value format (`MapState`).

 `DynamicKeyFunction` is actually a `BroadcastProcessFunction`.

```java
public abstract class BroadcastProcessFunction<IN1, IN2, OUT> {

    public abstract void processElement(IN1 value,
                                        ReadOnlyContext ctx,
                                        Collector<OUT> out) throws Exception;

    public abstract void processBroadcastElement(IN2 value,
                                                 Context ctx,
                                                 Collector<OUT> out) throws Exception;

}
```

The difference is the addition of the `processBroadcastElement` method through which messages of the rules stream will arrive. The following new version of `DynamicKeyFunction` allows modifying the list of data-distribution keys at runtime through this stream:

```java
public class DynamicKeyFunction
    extends BroadcastProcessFunction<Transaction, Rule, Keyed<Transaction, String, Integer>> {


  @Override
  public void processBroadcastElement(Rule rule,
                                     Context ctx,
                                     Collector<Keyed<Transaction, String, Integer>> out) {
    BroadcastState<Integer, Rule> broadcastState = ctx.getBroadcastState(RULES_STATE_DESCRIPTOR);
    broadcastState.put(rule.getRuleId(), rule);
  }

  @Override
  public void processElement(Transaction event,
                           ReadOnlyContext ctx,
                           Collector<Keyed<Transaction, String, Integer>> out){
    ReadOnlyBroadcastState<Integer, Rule> rulesState =
                                  ctx.getBroadcastState(RULES_STATE_DESCRIPTOR);
    for (Map.Entry<Integer, Rule> entry : rulesState.immutableEntries()) {
        final Rule rule = entry.getValue();
        out.collect(
          new Keyed<>(
            event, KeysExtractor.getKey(rule.getGroupingKeyNames(), event), rule.getRuleId()));
    }
  }
}
```

In the above code, `processElement()` receives Transactions, and `processBroadcastElement()` receives Rule updates. When a new rule is created, it is distributed as depicted in Figure 6 and saved in all parallel instances of the operator using `processBroadcastState`. We use a Rule’s ID as the key to store and reference individual rules. Instead of iterating over a hardcoded `List<Rules>`, we iterate over entries in the dynamically-updated broadcast state.

### Custom Window Processing

how you can use the "Swiss knife" of Flink - the [*Process Function*](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/process_function.html) to create an implementation that is tailor-made to match your streaming business logic requirements.

how you can implement your own **custom replacement for time windows** for cases where the out-of-the-box windowing available from the DataStream API does not satisfy your requirements. 

```java
public class SomeProcessFunction extends KeyedProcessFunction<KeyType, InputType, OutputType> {

	public void processElement(InputType event, Context ctx, Collector<OutputType> out){}

	public void onTimer(long timestamp, OnTimerContext ctx, Collector<OutputType> out) {}

	public void open(Configuration parameters){}
}
```

- `processElement()` receives input events one by one. You can react to each input by producing one or more output events to the next operator by calling `out.collect(someOutput)`. You can also pass data to a [side output](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/side_output.html) or ignore a particular input altogether.
- `onTimer()` is called by Flink when a previously-registered timer fires. Both event time and processing time timers are supported.
- `open()` is equivalent to a constructor. It is called inside of the [TaskManager’s](https://ci.apache.org/projects/flink/flink-docs-release-1.11/concepts/glossary.html#flink-taskmanager) JVM, and is used for initialization, such as registering Flink-managed state. It is also the right place to initialize fields that are not serializable and cannot be transferred from the JobManager’s JVM.

Most importantly, `ProcessFunction` also has access to the fault-tolerant state, handled by Flink. This combination, together with Flink's message processing and delivery guarantees, makes it possible to build resilient event-driven applications with almost arbitrarily sophisticated business logic. This includes creation and processing of custom windows with state.