### Samza

### Samza

Samza is a stream processing framework with the following features:

- **Simple API:** Unlike most low-level messaging system APIs, Samza provides a very simple callback-based “process message” API comparable to MapReduce.
- **Managed state:** Samza manages snapshotting and restoration of a stream processor’s state. When the processor is restarted, Samza restores its state to a consistent snapshot. Samza is built to handle large amounts of state (many gigabytes per partition).
- **Fault tolerance:** Whenever a machine in the cluster fails, Samza works with YARN to transparently migrate your tasks to another machine.
- **Durability:** Samza uses Kafka to guarantee that messages are processed in the order they were written to a partition, and that no messages are ever lost.
- **Scalability:** Samza is partitioned and distributed at every level. Kafka provides ordered, partitioned, replayable, fault-tolerant streams. YARN provides a distributed environment for Samza containers to run in.
- **Pluggable:** Though Samza works out of the box with Kafka and YARN, Samza provides a pluggable API that lets you run Samza with other messaging systems and execution environments.
- **Processor isolation:** Samza works with Apache YARN, which supports Hadoop’s security model, and resource isolation through Linux CGroups.


![](http://ox0pxbncm.bkt.clouddn.com/17-11-22/98296127.jpg)

### samza运行原理

![这里写图片描述](http://img.blog.csdn.net/20150918145544673)

Samza的Job是运行在Yarn上的。

架构图中展现的是提交2个Job示例。 
提交一个Job的基本过程： 
\-  Client将Job提交到RM（使用YarnClient 类）
\- RM根据当前资源分配Node Manager资源运行App Master
\- App Master向RM申请运行Container的Node Manager资源，得到资源后运行Container 
\- Container中执行你提交的Job

可以看出要在Yarn上运行一个Job需要有三样东西： 
\- Client：提交Job到Yarn上 
\- App Master：申请资源、启动Container 
\- Container：运行程序（我们写的处理程序就在这里运行）

Samza是如何将Job运行在Yarn上的

![Samza的Job提交过程](http://img.blog.csdn.net/20150917171623774)

1. 在Samza中是利用JobRunner提交Job到RM中的，**由于Samza对于Yarn、Kafka是可插拔的（pluggable）**，因此在此处调用了YarnJob类实际完成这个功能的。
2. Samza中的AppMaster就是SamzaAppMaster类，该类与RM通信，申请资源，与NM通信启动自己的Container—SamzaContainer
3. SamzaContainer启动后，会执行我们写的Task。

#### Client提交Job，启动SamzaAppMaster的过程

![SamzaAppMster启动的过程](http://img.blog.csdn.net/20150918151755538)

Samza是利用bin/run-job.sh提交Job的，例（hello-samza中提交Job）：

``` shell
deploy/samza/bin/run-job.sh --config-factory=org.apache.samza.config.factories.PropertiesConfigFactory --config-path=file://$PWD/deploy/samza/config/wikipedia-feed.properties
```

脚本实际调用org.apache.samza.job.JobRunner来解析配置文件，以及提交Job的。 
run-job.sh的内容：

``` shell
exec $(dirname $0)/run-class.sh **org.apache.samza.job.JobRunner** "$@"
```

由于配置文件中<code>job.factory.class=org.apache.samza.job.yarn.YarnJobFactory</code>(该处可以配置ProcessJobFactory或者ThreadJobFactory，但我只分析运行在Yarn上的情况)，因此在JobRunner中实际调用了YarnJob的submit,在submit中调用的是YarnClient的submitApplication方法，yarn提供api submitApplication将任务上传到yarn，这一块没有仔细研究，不过代码跟踪应该相似，yarn发送指令run-am.sh，创建一个appmaster，这个下一章讲。其中submitApplication中接收了一个类型为ApplicationSubmissionContext参数，其中主要包括package（我们写的程序的jar包）的path，以及启动SamzaAppMaster命令行。详细代码：

```scala
  def submit: YarnJob = {
    appId = client.submitApplication(/*实际调用的是YarnClient的submitApplication，YarnClient这个类主要是将job提交给Yarn的RM*/
      new Path(config.getPackagePath.getOrElse(throw new SamzaException("No YARN package path defined in config.")))/*APK的路径，①http可以下载的url②hdfs的路径*/,
      config.getAMContainerMaxMemoryMb.getOrElse(DEFAULT_AM_CONTAINER_MEM),
      1,
      List(
        "export SAMZA_LOG_DIR=%s && ln -sfn %s logs && exec ./__package/bin/run-am.sh 1>logs/%s 2>logs/%s"/*Yarn的RM会执行run-am.sh来启动Samza的SamzaAppMaster*/
          format (ApplicationConstants.LOG_DIR_EXPANSION_VAR, ApplicationConstants.LOG_DIR_EXPANSION_VAR, ApplicationConstants.STDOUT, ApplicationConstants.STDERR)),
      Some({
        val envMap = Map(
          ShellCommandConfig.ENV_CONFIG -> Util.envVarEscape(SamzaObjectMapper.getObjectMapper.writeValueAsString(config)),
          ShellCommandConfig.ENV_JAVA_OPTS -> Util.envVarEscape(config.getAmOpts.getOrElse("")))
        val envMapWithJavaHome = config.getAMJavaHome match {
          case Some(javaHome) => envMap + (ShellCommandConfig.ENV_JAVA_HOME -> javaHome)/*一些环境变量*/
          case None => envMap
        }
        envMapWithJavaHome
      }),

      Some("%s_%s" format (config.getName.get, config.getJobId.getOrElse(1))))

    this
  }
```

```scala
appCtx.setAMContainerSpec(containerCtx)
appCtx.setApplicationType(ClientHelper.applicationType)
info("submitting application request for %s" format appId.get)
yarnClient.submitApplication(appCtx)
```

到目前为止，已经将我们的Job提交给Yarn的RM，并且RM可以启动SamzaAppMaster。

#### SamzaAppMaster申请资源、启动Container的过程

SamzaAppMaster实现了 AMRMClientAsync.CallbackHandler接口。该接口的作用是AppMaster向Yarn的RM申请资源成功后的回调函数，用以通知SamzaAppMaster。SamzaAppMaster接到通知后通知Yarn的Node Manerger来启动SamzaContainer，这样我们的Job就会被执行了。 

![SamzaContainer启动过程](http://img.blog.csdn.net/20150918142727408)

``` scala
object SamzaAppMaster extends Logging with AMRMClientAsync.CallbackHandler {
...
val amClient = AMRMClientAsync.createAMRMClientAsync[ContainerRequest](interval, this)/*this:将自己注册给Yarn的RM，待资源申请成功后会调用自己类的onContainersAllocated*/
...
      val state = new SamzaAppMasterState(-1, containerId, nodeHostString, nodePortString.toInt, nodeHttpPortString.toInt)
      val service = new SamzaAppMasterService(config, state, registry, clientHelper)
      val lifecycle = new SamzaAppMasterLifecycle(containerMem, containerCpu, state, amClient)
      val metrics = new SamzaAppMasterMetrics(config, state, registry)
      val am = new SamzaAppMasterTaskManager({ System.currentTimeMillis }, config, state, amClient, hConfig)/*这个类主要与NM通信，并启动Container*/
      listeners = List(state, service, lifecycle, metrics, am)
      run(amClient, listeners, hConfig, interval)/**/
}

 def run(amClient: AMRMClientAsync[ContainerRequest], listeners: List[YarnAppMasterListener], hConfig: YarnConfiguration, interval: Int): Unit = {
    try {
      amClient.init(hConfig)
      amClient.start
      listeners.foreach(_.onInit)/*此处启动所有监听和服务，也启动了SamzaAppMasterTaskManager类*/
      ...
}


  override def onContainersAllocated(containers: java.util.List[Container]): Unit =
    containers.foreach(container => listeners.foreach(_.onContainerAllocated(container)))/*资源申请成功，Yarn 的RM的回调（图中的3），真正启动SamzaContainer的是SamzaAppMasterTaskManager类中的onContainerAllocated函数*/

--------------------------------------------------------------------
SamzaAppMasterTaskManager类：
override def onInit() {
    state.neededContainers = state.taskCount
    state.unclaimedTasks = state.jobCoordinator.jobModel.getContainers.keySet.map(_.toInt).toSet
    containerManager = NMClient.createNMClient()
    containerManager.init(conf)
    containerManager.start/*建立与NM的通信*/

    info("Requesting %s containers" format state.taskCount)
requestContainers(config.getContainerMaxMemoryMb.getOrElse(DEFAULT_CONTAINER_MEM), config.getContainerMaxCpuCores.getOrElse(DEFAULT_CPU_CORES), state.neededContainers)/*向RM申请资源（图中的1.1）*/
  }

override def onContainerAllocated(container: Container) {/*Container申请成功后的回调函数*/
...
 val cmdBuilderClassName = config.getCommandClass.getOrElse(classOf[ShellCommandBuilder].getName)/*获得ShellCommandBuilder类，此类中包含启动SamzaContainer的脚本命令*/
        val cmdBuilder = Class.forName(cmdBuilderClassName).newInstance.asInstanceOf[CommandBuilder]
          .setConfig(config)
          .setId(taskId)
          .setUrl(state.coordinatorUrl)
        val command = cmdBuilder.buildCommand/*得到的就是bin/run-container.sh*/
...
 val cmdBuilderClassName = config.getCommandClass.getOrElse(classOf[ShellCommandBuilder].getName)
        val cmdBuilder = Class.forName(cmdBuilderClassName).newInstance.asInstanceOf[CommandBuilder]
          .setConfig(config)
          .setId(taskId)
          .setUrl(state.coordinatorUrl)
        val command = cmdBuilder.buildCommand
...

 startContainer(/*调用内部函数，实际调用的是NMClient.createNMClient().startContainer()*/
          path/*我们提交的PKG的路径*/,
          container,
          env.toMap,
          "export SAMZA_LOG_DIR=%s && ln -sfn %s logs && exec ./__package/%s 1>logs/%s 2>logs/%s" format (ApplicationConstants.LOG_DIR_EXPANSION_VAR, ApplicationConstants.LOG_DIR_EXPANSION_VAR, command/*此处将启动SamzaContainer的脚本传递给NM，NM得到PKG的路径后将PKG解压，之后就可以执行脚本启动Container*/, ApplicationConstants.STDOUT, ApplicationConstants.STDERR))

}


 protected def startContainer(packagePath: Path, container: Container, env: Map[String, String], cmds: String*) {
 ...
   containerManager.startContainer(container, ctx)
 ...
  }
```

#### SamzaContainer运行Task的过程

具体执行的过程SamzaContainer–>main–>safeMain–>apply–>run–>RunLoop.run–>RunLoop.process 
解释： 
1、main: main是默认被执行的，里面直接调用了safeMain 
2、safeMain: 在safeMain最后调用了Object的SamzaContainer(containerModel, jobModel).run 
3、apply: 由于SamzaContainer是Object，那么首先会默认会执行apply，在apply中主要是对一些变量赋值、初始化，包括初始化Task实例 
4、run: 在run中主要开始对消息开始轮训处理 
5、RunLoop.run: RunLoop.run这个是真正向process里读取数据的线程 
6、RunLoop.process: RunLoop.process会最终调用我们写的函数

SamzaContainer源码到apply：

```scala
object SamzaContainer extends Logging {
  def main(args: Array[String]) {
    safeMain(() => new JmxServer, new SamzaContainerExceptionHandler(() => System.exit(1)))
  }

  def safeMain{
   ...
      jmxServer = newJmxServer()
      SamzaContainer(containerModel, jobModel).run/*这里开始执行了*/
   ...
  }
/*---------------------------------------------------*/
/*apply函数：初始化Task实例*/
def apply(containerModel: ContainerModel, jobModel: JobModel) = {
...
    val taskClassName = config
      .getTaskClass
      .getOrElse(throw new SamzaException("No task class defined in configuration.")) /*获取配置文件中的val TASK_CLASS = "task.class" // streaming.task-factory-class*/
...
/*下面就是获取Task的实例*/
 val taskInstances: Map[TaskName, TaskInstance] = containerModel.getTasks.values.map(taskModel => {
      debug("Setting up task instance: %s" format taskModel)
      val taskName = taskModel.getTaskName
      val task = Util.getObj[StreamTask](taskClassName)
      val taskInstanceMetrics = new TaskInstanceMetrics("TaskName-%s" format taskName)
      val collector = new TaskInstanceCollector(producerMultiplexer, taskInstanceMetrics)
      val storeConsumers = changeLogSystemStreams
        .map {
          case (storeName, changeLogSystemStream) =>
            val systemConsumer = systemFactories
              .getOrElse(changeLogSystemStream.getSystem, throw new SamzaException("Changelog system %s for store %s does not exist in the config." format (changeLogSystemStream, storeName)))
              .getConsumer(changeLogSystemStream.getSystem, config, taskInstanceMetrics.registry)
            (storeName, systemConsumer)
        }.toMap
   ...
   /*真正的Task生成了*/
      val taskInstance = new TaskInstance(
        task = task,
        taskName = taskName,/*这个获取的就是配置文件中task.class声明的类*/
        config = config,
        metrics = taskInstanceMetrics,
        consumerMultiplexer = consumerMultiplexer,
        collector = collector,
        offsetManager = offsetManager,
        storageManager = storageManager,
        reporters = reporters,
        systemStreamPartitions = systemStreamPartitions,
        exceptionHandler = TaskInstanceExceptionHandler(taskInstanceMetrics, config))

      (taskName, taskInstance)
    }).toMap
  }
```

run函数：在run中主要开始对消息开始轮训处理 

```java
def run {
    try {
      info("Starting container.")

/*启动&初始化进程*/
      startMetrics
      startOffsetManager
      startStores
      startProducers
      startTask
      startConsumers

      info("Entering run loop.")
      runLoop.run/*这个是真正的获取Kafka中的内容，也就是我写的process就在这里*/
    } catch {
      case e: Exception =>
        error("Caught exception in process loop.", e)
        throw e
    } finally {
      info("Shutting down.")

      shutdownConsumers
      shutdownTask
      shutdownProducers
      shutdownStores
      shutdownOffsetManager
      shutdownMetrics

      info("Shutdown complete.")
    }
  }
```

Looper.run函数：这个是真正向process里读取数据的线程 

```scala
def run {
    addShutdownHook(Thread.currentThread())

    while (!shutdownNow) {
      process  /*找到了，在这里*/
      window
      commit
    }
}
```

process函数：RunLoop.process会最终调用我们写的函数

```scala
private def process {
    trace("Attempting to choose a message to process.")
    metrics.processes.inc

    updateTimer(metrics.processMs) {
      val envelope = updateTimer(metrics.chooseMs) {
        consumerMultiplexer.choose
      }

      if (envelope != null) {
        val ssp = envelope.getSystemStreamPartition

        trace("Processing incoming message envelope for SSP %s." format ssp)
        metrics.envelopes.inc

        val taskInstance = systemStreamPartitionToTaskInstance(ssp)
        val coordinator = new ReadableCoordinator(taskInstance.taskName)

        taskInstance.process(envelope, coordinator)/*是不是很熟悉了，我们写的程序终于被调用了*/
        checkCoordinator(coordinator)
      } else {
        trace("No incoming message envelope was available.")
        metrics.nullEnvelopes.inc
      }
    }
}
```

到此我们的程序在Yarn上已经运行了。

### 结构

Samza is made up of three layers:

1. A streaming layer. Kafka
2. An execution layer. YARN
3. A processing layer. Samza API(现在的理解是samza就是一个库，提供了api使kafka能够数据流处理)

![diagram-medium](http://samza.apache.org/img/0.13/learn/documentation/introduction/samza-ecosystem.png)

和hadoop的模式有点相似：

![diagram-medium](http://samza.apache.org/img/0.13/learn/documentation/introduction/samza-hadoop.png)

官网说Samza不局限于YARN和Kafka，应该就指的他的可插入性（pluggable）吧。

官网samza运行流程图：

client向RM发送开始一个samza job的请求
RM接收请求向一个NM发送指令，分配资源，创建一个AppMaster
AppMaster与RM通信，请求创建container跑task，RM向一个NM发送指令创建samza task runner
samza task runner将在broker上创建任务，可以在多个broker上面创建。

![diagram-small](http://samza.apache.org/img/0.13/learn/documentation/introduction/samza-yarn-kafka-integration.png)

比如一个例子，SELECT user_id, COUNT(*) FROM PageViewEvent GROUP BY user_id，计算每个userid的页面查看次数，将会跑两个任务，一个任务负责groupby的任务，将关键字聚合，发送同一个ID的消息到一个中间topic的一个分区中，另一个任务负责消费topic进行统计，第二个任务中的每个task消费中间topic的一个分区，这样建立一个计数器，进行统计。这样就类似于一个wordcount的功能了吧？想想成统计一篇文章，将所有的单词拆分成一个ID，放入topic的partition，让consumer中task来计算。（**注意job和task的区别**，理解为一个是作业，包含多个任务）

![重新划分GROUP BY](http://samza.apache.org/img/0.13/learn/documentation/introduction/group-by-example.png)

### 创建一个samza项目

编写pom.xml，添加依赖，使用maven进行打包。

示例：

config.properties

```properties
#Job
#指定运行环境，本地测试使用ThreadJobFactory或ProcessJobFactory
#job.factory.class=org.apache.samza.job.yarn.YarnJobFactory
job.factory.class=org.apache.samza.job.local.ThreadJobFactory
job.name=CountLatancy

#YARN
#本地运行时可以省略
yarn.package.path=file://{tmp}/filename.tar.gz
#Task
#主类名，包名.主类名
task.class=samza.CountLatancy
#使用kafka作为输入流时,字段含义为kafka.topic
task.inputs=kafka.clickevent
#windowable job 必须配置，单位为ms
task.window.ms=60000

#Serializers
#定义序列化与反序列化类
serializers.registry.json.class=org.apache.samza.serializers.JsonSerdeFactory
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory
serializers.registry.integer.class=org.apache.samza.serializers.IntegerSerdeFactory

# Kafka System
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
#输入流的键值序列化时使用哪一个序列化类。
systems.kafka.samza.key.serde=string
systems.kafka.samza.msg.serde=string
systems.kafka.consumer.zookeeper.connect=localhost:2181
systems.kafka.producer.bootstrap.servers=localhost:9092


# Job Coordinator
job.coordinator.system=kafka
# Normally, this would be 3, but we have only one broker.
job.coordinator.replication.factor=1
```

samzaDemo.java

```java
package samza;

import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.samza.config.Config;
import org.apache.samza.system.IncomingMessageEnvelope;
import org.apache.samza.system.OutgoingMessageEnvelope;
import org.apache.samza.system.SystemStream;
import org.apache.samza.task.InitableTask;
import org.apache.samza.task.MessageCollector;
import org.apache.samza.task.StreamTask;
import org.apache.samza.task.TaskContext;
import org.apache.samza.task.TaskCoordinator;
import org.apache.samza.task.WindowableTask;
import org.json.JSONObject;

public class CountLatancy implements StreamTask,WindowableTask,InitableTask{
    Map<String,Integer> clickEvent = new HashMap<String,Integer>();
    Map<String,Integer> clickCount = new HashMap<String,Integer>();
    int total = 0;
    private static String TOPIC_NAME = "pageviews";

    private static SystemStream stream = new SystemStream("kafka",TOPIC_NAME);

    public void window(MessageCollector collector, TaskCoordinator coodinator)
            throws Exception {

        //int total = 0;
        for(Entry<String,Integer> entry:clickEvent.entrySet()){
            String name = entry.getKey();
            int latancy = entry.getValue();
            int count = clickCount.get(name);
            String json = "{\"name\" : \""+name+"\""+
                    ",\"total_latancyMs\" : "+latancy+
                    ",\"clickNum\" : "+count+
                    ",\"aver_lantancyMs\" : "+(latancy+0.0)/count+
                    "}";
            total+=count;
            collector.send(new OutgoingMessageEnvelope (stream,json));
        }
        collector.send(new OutgoingMessageEnvelope (stream,"Total messages per window : "+total+" ."));
        clickEvent = new HashMap<String,Integer>();
        clickCount = new HashMap<String,Integer>();
    }

    public void process(IncomingMessageEnvelope envelope, MessageCollector collector,
            TaskCoordinator coodinator) throws Exception {
        JSONObject json = new JSONObject(envelope.getMessage().toString());
        String name = json.getString("name");
        int latancy = json.getInt("latancyMs");
        if(clickEvent.containsKey(json.get("name").toString())){
            clickEvent.put(name, clickEvent.get(name)+latancy);
            clickCount.put(name, clickCount.get(name)+1);
        }else{
            clickEvent.put(name,latancy);
            clickCount.put(name,1);
        }
    }

    public void init(Config config, TaskContext task) throws Exception {
        System.out.println("----------------------------------------------------------------");
        System.out.println(config.toString());
        System.out.println("----------------------------------------------------------------");

    }
}
```

pom.xml

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>cn.myc</groupId>
  <artifactId>samzademo</artifactId>
  <version>0.0.1</version>
  <packaging>jar</packaging>

  <name>samzademo</name>
  <url>http://maven.apache.org</url>

  <properties>
    <!-- maven specific properties -->
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <samza.version>0.13.0</samza.version>
  </properties>

  <dependencies>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-api</artifactId>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-core_2.11</artifactId>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-log4j</artifactId>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-shell</artifactId>
      <classifier>dist</classifier>
      <type>tgz</type>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-yarn_2.11</artifactId>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-kv_2.11</artifactId>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-kv-rocksdb_2.11</artifactId>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.samza</groupId>
      <artifactId>samza-kafka_2.11</artifactId>
      <version>${samza.version}</version>
    </dependency>
    <dependency>
	    <groupId>org.apache.kafka</groupId>
	    <artifactId>kafka_2.11</artifactId>
    	<version>1.0.0</version>
	  </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
      <version>1.6.2</version>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.6.2</version>
    </dependency>
    <dependency>
	    <groupId>org.json</groupId>
	    <artifactId>json</artifactId>
	    <version>20171018</version>
	</dependency>
  </dependencies>

  <repositories>
    <repository>
      <id>my-local-repo</id>
      <url>file://${user.home}/.m2/repository</url>
    </repository>
    <repository>
      <id>apache-releases</id>
      <url>https://repository.apache.org/content/groups/public</url>
    </repository>
    <repository>
      <id>scala-tools.org</id>
      <name>Scala-tools Maven2 Repository</name>
      <url>https://oss.sonatype.org/content/groups/scala-tools</url>
    </repository>
    <repository>
      <id>cloudera-repos</id>
      <name>Cloudera Repos</name>
      <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>
    </repository>
  </repositories>

  <build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>3.1</version>
        <configuration>
          <source>1.6</source>
          <target>1.6</target>
        </configuration>
      </plugin>
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <version>2.3</version>
        <configuration>
          <descriptors>
            <descriptor>src/main/assembly/src.xml</descriptor>
          </descriptors>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>
</project>

```



代码是统计一下固定时间段内，每个人点击总点击次数和总时延。输入流demo为：

```
{"timestamp":"2015-09-01T01:35:19Z","url":"/foo","user":"bob","latencyMs":66}
```

创建一个maven工程，工程的结构和hello-samza的相似，config和resource可以移植过来。

将上述文件创建并加入到相似路径下，执行下述命令：

```shell
cd {src_home}
mvn clean package
cp target/target.jar {samza_home}/lib/
cd {samza_home}
tar -zcvf filename.tar.gz *
mv filename.tar.gz {tmp}/
{samza_home}/bin/run-job.sh \
--config-factory=org.apache.samza.config.factories.PropertiesConfigFactory \
--config-path=file://{samza_home}/config/config.properties
```

工程运行完成。

### 学习成果

目前来看samza的生态还比较年轻，落地的情况还是太少，由于自己知识栈的缺失，对于如何自己编写一个samza的程序存在问题，比如maven打包，示例代码目前没有拆开运行，导致对于每一步的关键输出不了解，整个代码缺少必要信息的了解必然是不好进行下一步学习的，目前来看难点有二：

1. maven不熟悉。
2. samza代码修改不容易，不会在关键信息上进行输出查看。
3. kafka这一块基本不懂，没有开发过producer和consumer，不知道分布式模式下的工作机制

总的来说，samza的学习成本其实还是挺高的，所以说这周先总结，让老师评价一下，然后吧storm搭建好，一起评估一下，最后决定用什么，我还是比较倾向于用spark。